{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 休息一下\n",
    "\n",
    "\n",
    "## 下午場開始\n",
    "\n",
    "目錄：\n",
    "\n",
    "1. 斷詞\n",
    "    * 安裝結巴\n",
    "    * 斷詞原理簡單講\n",
    "    * 下載字典\n",
    "2. 貝氏分類器\n",
    "    * 原理\n",
    "    * 卡方\n",
    "    * 情緒字典\n",
    "3. 實作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安裝結巴\n",
    "\n",
    "> pip install jieba\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 斷詞原理簡單講\n",
    "\n",
    "首先，知道詞跟詞出現在上下文的機率  \n",
    "透過viterbi等演算法實現HMM模型  \n",
    "找出機率最高的斷詞組合  \n",
    "\n",
    "![img](jieba_procedure.png)\n",
    "\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/7/73/Viterbi_animated_demo.gif)\n",
    "\n",
    "斷詞，需要知道每個字：\n",
    "1. S(獨立成詞)、B（詞的開頭）、M（中間）、E（結尾）四種詞的狀態的機率\n",
    "\n",
    "如此就能算出機率最大的斷詞組合\n",
    "\n",
    "![img](viterbi.png)\n",
    "圖片引用自 [中文斷詞：斷句不要悲劇](http://s.itho.me/techtalk/2017/%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E%EF%BC%9A%E6%96%B7%E5%8F%A5%E4%B8%8D%E8%A6%81%E6%82%B2%E5%8A%87.pdf)\n",
    "\n",
    "### 以下用WIKI百科上的viterbi做示範（參考即可）\n",
    "\n",
    "[wiki -viterbi](https://zh.wikipedia.org/wiki/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95)\n",
    "\n",
    "使用viterbi時  \n",
    "需要先知道上一個狀態變化到下一個狀態的機率  \n",
    "以及每個狀態的發生機率是多少  \n",
    "wiki是以醫生看病當例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states = ('Healthy', 'Fever')\n",
    " \n",
    "observations = ('normal', 'cold', 'dizzy')\n",
    " \n",
    "start_probability = {'Healthy': 0.6, 'Fever': 0.4}\n",
    " \n",
    "transition_probability = {\n",
    "   'Healthy' : {'Healthy': 0.7, 'Fever': 0.3},\n",
    "   'Fever' : {'Healthy': 0.4, 'Fever': 0.6},\n",
    "   }\n",
    " \n",
    "emission_probability = {\n",
    "   'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n",
    "   'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6},\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \n",
      "       0       1       2\n",
      "Fever: 0.04000 0.02700 0.01512 \n",
      "Healt: 0.30000 0.08400 0.00588 \n",
      "(0.01512, ['Healthy', 'Healthy', 'Fever'])\n"
     ]
    }
   ],
   "source": [
    "# Helps visualize the steps of Viterbi.\n",
    "def print_dptable(V):\n",
    "    print(\"    \")\n",
    "    for i in range(len(V)):\n",
    "        print(\"%8d\" % i, end='')\n",
    "    print()\n",
    "\n",
    "    for y in V[0].keys():\n",
    "        print(\"%.5s: \" % y, end=\"\")\n",
    "        for t in range(len(V)):\n",
    "            print(\"%.7s\" % (\"%f\" % V[t][y]), end=\" \")\n",
    "        print()\n",
    "\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    Pro = [{}]\n",
    "    path = {}\n",
    "\n",
    "    for s in states:\n",
    "        Pro[0][s] = start_p[s] * emit_p[s][obs[0]]\n",
    "        path[s] = [s]\n",
    "\n",
    "    for index in range(1, len(obs)):\n",
    "        Pro.append({})\n",
    "        newPath = {}\n",
    "        for newstate in states:\n",
    "            prob, state = max([ (Pro[index-1][oldState] * trans_p[oldState][newstate] * emit_p[newstate][obs[index]], oldState) for oldState in states])\n",
    "\n",
    "            Pro[index][newstate] = prob\n",
    "            newPath[newstate] = path[state] + [newstate]\n",
    "        path = newPath\n",
    "\n",
    "    print_dptable(Pro)\n",
    "    prob, state = max([(value, key) for key, value in Pro[-1].items()])\n",
    "    return prob, path[state]\n",
    "\n",
    "def example():\n",
    "    return viterbi(observations,\n",
    "                   states,\n",
    "                   start_probability,\n",
    "                   transition_probability,\n",
    "                   emission_probability)\n",
    "print(example())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 斷詞示範"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\udic\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 3.743 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吉林市', '長', '春藥店']\n"
     ]
    }
   ],
   "source": [
    "import jieba, os\n",
    "print(jieba.lcut('吉林市長春藥店'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下載字典 \n",
    "\n",
    "答案不是 ~~春藥店~~  \n",
    "是**長春** **藥店**  \n",
    "但是蒐集到的單字不夠多  \n",
    "導致演算法覺得這種組合的機率很小  \n",
    "\n",
    "要改善就需要額外的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吉林市', '長春', '藥店']\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict(os.path.join('', 'dictionary', 'dict.txt.big.txt'))\n",
    "jieba.load_userdict(os.path.join('', \"dictionary\", \"NameDict_Ch_v2\"))\n",
    "print(jieba.lcut('吉林市長春藥店'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 貝氏分類器\n",
    "\n",
    "#### 理論\n",
    "\n",
    "貝氏定理就是我們熟知的條件機率  \n",
    "\n",
    "首先呢：\n",
    "![img](貝氏1.png)\n",
    "\n",
    "倒過來也一樣\n",
    "![img](貝氏2.png)\n",
    "\n",
    "所以兩式個關係是這樣：\n",
    "![img](貝氏3.png)\n",
    "\n",
    "這個關係式，可以用於分類上面  \n",
    "該公式解釋成白話文，意思是：\n",
    "1. 如果有出現這些字，讓他屬於某一類別的機率 == （該類別底下，有出現這些字的機率）* 該類別出現的機率 / 出現這些字的機率\n",
    "![img](貝氏4.png)\n",
    "\n",
    "### 問題是...\n",
    "\n",
    "<mark style='color:red'>該類別底下，有出現這些字的機率</mark>  \n",
    "e.q. 請計算負面句子當中，同時出現好棒棒、廠廠、三寶、酸民的機率  \n",
    "若訓練資料裏面，沒有同時出現 <mark style='color:red'>好棒棒、廠廠、三寶、酸民</mark>的句子  \n",
    "那他屬於負面句子的機率是0  \n",
    "正面的句子也是0（我不相信正面句子會講什麼三寶）  \n",
    "最後判斷會淪為猜測（導致準確度趨近0.5）  \n",
    "\n",
    "![img](naiveB.png)\n",
    "![img](naiveB1.png)\n",
    "\n",
    "所以如果我們拿掉 <mark style='color:red'>同時出現</mark>這個constraint呢？</mark>  \n",
    "假設這些字出現的機率為獨立事件  \n",
    "則我們可以將公式改寫成\n",
    "![img](naiveB2.png)\n",
    "![img](naiveB3.png)\n",
    "\n",
    "這就是今天所使用的 NaiveBayes \n",
    "\n",
    "首先要先自制兩個函式  \n",
    "會幫我們對資料進行前處理\n",
    "1. create_Mainfeatures：\n",
    "    * 將正面與反正資料串在一起\n",
    "    * 計算每個單字出現的頻率\n",
    "    * 利用卡方公式，如果該單字經常出現在正面文集或是負面文集，就是情緒性的單字\n",
    "    * 將情緒性的單字集成字典並回傳 -> 就是 bestMainFeatures\n",
    "2. CutAndrmStopWords：\n",
    "    * 輸入一個句子\n",
    "    * 使用結巴斷詞\n",
    "    * 也移除stopwords\n",
    "    * 將結果回傳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp950' codec can't decode byte 0xe2 in position 2: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-197facff1bc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mBASEDIR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASEDIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stopwords.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_userdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASEDIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dictionary'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dict.txt.big.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_userdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASEDIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dictionary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"NameDict_Ch_v2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\udic\\appdata\\local\\programs\\python\\python35\\Lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \"\"\"\n\u001b[1;32m--> 265\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp950' codec can't decode byte 0xe2 in position 2: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import itertools, pickle, json, sys\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "def create_Mainfeatures(pos_data, neg_data, BestFeatureVec):\n",
    "    posWords = list(itertools.chain(*pos_data)) #把多為數組解煉成一維數組\n",
    "    negWords = list(itertools.chain(*neg_data)) #同理\n",
    "\n",
    "    # bigram\n",
    "    bigram_finder = BigramCollocationFinder.from_words(posWords)\n",
    "    posBigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 5000)\n",
    "    bigram_finder = BigramCollocationFinder.from_words(negWords)\n",
    "    negBigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 5000)\n",
    "    posWords += posBigrams #詞和雙詞搭配\n",
    "    negWords += negBigrams\n",
    "\n",
    "    word_fd = FreqDist() #可統計所有詞的詞頻\n",
    "    cond_word_fd = ConditionalFreqDist() #可統計積極文本中的詞頻和消極文本中的詞頻\n",
    "    for word in posWords:\n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['pos'][word] += 1\n",
    "    for word in negWords:\n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['neg'][word] += 1\n",
    "\n",
    "    pos_word_count = cond_word_fd['pos'].N() #積極詞的數量\n",
    "    neg_word_count = cond_word_fd['neg'].N() #消極詞的數量\n",
    "    total_word_count = pos_word_count + neg_word_count\n",
    "\n",
    "    word_features = {}\n",
    "    for word, freq in word_fd.items():\n",
    "        pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word], (freq, pos_word_count), total_word_count) #計算積極詞的卡方統計量，這裏也可以計算互信息等其它統計量\n",
    "        neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word], (freq, neg_word_count), total_word_count) #同理\n",
    "        word_features[word] = pos_score + neg_score\n",
    "\n",
    "    def find_best_words(number):\n",
    "        best = sorted(word_features.items(), key=lambda x: -x[1])[:number] # 把詞按信息量倒序排序。number 是特徵的微度，式可以不斷調整至最優的\n",
    "        return set(w for w, s in best)\n",
    "\n",
    "    best = find_best_words(BestFeatureVec)\n",
    "    pickle.dump(best, open('bestMainFeatures.pickle.{}'.format(BestFeatureVec), 'wb'))\n",
    "    return best\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "import jieba, os\n",
    "\n",
    "BASEDIR = os.path.dirname('.')\n",
    "stopwords = json.load(open(os.path.join(BASEDIR, 'stopwords', 'stopwords.json'), 'r'))\n",
    "jieba.load_userdict(os.path.join(BASEDIR, 'dictionary', 'dict.txt.big.txt'))\n",
    "jieba.load_userdict(os.path.join(BASEDIR, \"dictionary\", \"NameDict_Ch_v2\"))\n",
    "def CutAndrmStopWords(sentence):\n",
    "    def condition(x):\n",
    "        x = list(x)\n",
    "        word, flag = x[0], x[1]\n",
    "        if len(word) > 1 and flag!='eng' and flag != 'm' and flag !='mq' and word not in stopwords:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    result = filter(condition, pseg.cut(sentence))\n",
    "    result = map(lambda x:list(x)[0], result)\n",
    "    return list(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類器的演算法\n",
    "\n",
    "建立一個叫作swinger的類別  \n",
    "以下解釋函式功能\n",
    "1. load函式：\n",
    "    * 把訓練資料載入\n",
    "    * 透過前面建立好的create_Mainfeatures，從訓練資料中找出最好的情緒字典，best main features\n",
    "    * 透過bestMainFeatures，把訓練資料的句字去蕪存菁，再送入分類器做訓練\n",
    "2. buildTestData：\n",
    "    * 將測試資料去蕪存菁\n",
    "3. best_Mainfeatures：\n",
    "    * 使用bestMainFeatures，將句子去蕪存菁的函式\n",
    "4. score：\n",
    "    * 用測試資料去算準確度\n",
    "5. swing：\n",
    "    * 分類的api，給一句話，他會依據模型去判斷pos或是neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a36cc68f70ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscikitlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSklearnClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNuSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\udic\\documents\\htdocs\\publictutorial\\venv\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[1;31m# avoid flakes unused variable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\udic\\documents\\htdocs\\publictutorial\\venv\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import nltk, json, pickle, sys, collections, jieba, os\n",
    "from random import shuffle\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure, log_likelihood, approxrand)\n",
    "\n",
    "\n",
    "class Swinger(object):\n",
    "    \"\"\"docstring for Swinger\"\"\"\n",
    "    classifier_table = {\n",
    "        'MultinomialNB':MultinomialNB(),\n",
    "        'BernoulliNB':BernoulliNB(),\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = []\n",
    "        self.test = []\n",
    "        self.classifier = ''\n",
    "\n",
    "    def load(self, model, pos, neg, BestFeatureVec=700):\n",
    "        BestFeatureVec = int(BestFeatureVec)\n",
    "\n",
    "        print('load bestMainFeatures failed!!\\nstart creating bestMainFeatures ...')\n",
    "\n",
    "        self.pos_origin = json.load(open(pos, 'r'))\n",
    "        self.neg_origin = json.load(open(neg, 'r'))\n",
    "        shuffle(self.pos_origin)\n",
    "        shuffle(self.neg_origin)\n",
    "        poslen = len(self.pos_origin)\n",
    "        neglen = len(self.neg_origin)\n",
    "\n",
    "        # build train and test data.\n",
    "        self.pos_review = self.pos_origin[:int(poslen*0.9)]\n",
    "        self.pos_test = self.pos_origin[int(poslen*0.9):]\n",
    "        self.neg_review = self.neg_origin[:int(neglen*0.9)]\n",
    "        self.neg_test = self.neg_origin[int(neglen*0.9):]\n",
    "\n",
    "        self.bestMainFeatures = create_Mainfeatures(pos_data=self.pos_review, neg_data=self.neg_review, BestFeatureVec=BestFeatureVec) # 使用詞和雙詞搭配作為特徵\n",
    "        print(self.bestMainFeatures)\n",
    "        # build model\n",
    "        print('start building {} model!!!'.format(model))\n",
    "\n",
    "        self.classifier = SklearnClassifier(self.classifier_table[model]) #nltk在sklearn的接口\n",
    "        if len(self.train) == 0:\n",
    "            print('build training data')\n",
    "            posFeatures = self.emotion_features(self.best_Mainfeatures, self.pos_review, 'pos')\n",
    "            negFeatures = self.emotion_features(self.best_Mainfeatures, self.neg_review, 'neg')\n",
    "            self.train = posFeatures + negFeatures\n",
    "        self.classifier.train(self.train) #訓練分類器\n",
    "        pickle.dump(self.classifier, open('{}.pickle.{}'.format(model, BestFeatureVec),'wb'))\n",
    "\n",
    "    def buildTestData(self, pos_test, neg_test):\n",
    "        pos_test = json.load(open(pos_test, 'r'))\n",
    "        neg_test = json.load(open(neg_test, 'r'))\n",
    "        posFeatures = self.emotion_features(self.best_Mainfeatures, pos_test, 'pos')\n",
    "        negFeatures = self.emotion_features(self.best_Mainfeatures, neg_test, 'neg')\n",
    "        return posFeatures + negFeatures\n",
    "\n",
    "    def best_Mainfeatures(self, word_list):\n",
    "        return {word:True for word in word_list if word in self.bestMainFeatures}\n",
    "\n",
    "    def score(self, pos_test, neg_test):\n",
    "        from sklearn.metrics import precision_recall_curve\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.metrics import auc\n",
    "        # build test data set\n",
    "        if len(self.test) == 0:\n",
    "            self.test = self.buildTestData(pos_test, neg_test)\n",
    "\n",
    "        test, test_tag = zip(*self.test)\n",
    "        pred = list(map(lambda x:1 if x=='pos' else 0, self.classifier.classify_many(test))) #對開發測試集的數據進行分類，給出預測的標籤\n",
    "        tag = list(map(lambda x:1 if x=='pos' else 0, test_tag))\n",
    "        # ROC AUC\n",
    "        fpr, tpr, _ = roc_curve(tag, pred, pos_label=1)\n",
    "        print(\"ROC AUC:\" + str(auc(fpr, tpr)))\n",
    "        return auc(fpr, tpr)\n",
    "\n",
    "    def emotion_features(self, feature_extraction_method, data, emo):\n",
    "        return list(map(lambda x:[feature_extraction_method(x), emo], data)) #爲積極文本賦予\"pos\"\n",
    "\n",
    "    def swing(self, sentence):\n",
    "        sentence = self.best_Mainfeatures(CutAndrmStopWords(sentence))\n",
    "        return self.classifier.classify(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB V.S. BernoulliNB\n",
    "都是Naive Bayes的一種  \n",
    "差異在於：\n",
    "1. Multinomial 會計算該單字出現再該類別幾次\n",
    "2. Bernoulli 只是計算該單字出現與否而已\n",
    "\n",
    "通常Multinomial會更適合用在Text classification上面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = Swinger()\n",
    "s.load('MultinomialNB', pos='pos.json', neg='neg.json', BestFeatureVec=10)\n",
    "s.score(pos_test='pos.json', neg_test='neg.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = Swinger()\n",
    "s.load('BernoulliNB', pos='pos.json', neg='neg.json', BestFeatureVec=10)\n",
    "s.score(pos_test='pos.json', neg_test='neg.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.swing('大停電的夜晚，我很幸運看到了星空')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.swing('XXX 停電害我不能打電動拉')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同的feature數量對準確度的影響?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "multi = []\n",
    "bernou = []\n",
    "for num in range(10, 50, 10):\n",
    "    s = Swinger()\n",
    "    s.load('MultinomialNB', pos='pos.json', neg='neg.json', BestFeatureVec=num)\n",
    "    multi.append(s.score(pos_test='pos.json', neg_test='neg.json'))\n",
    "    \n",
    "    s.load('BernoulliNB', pos='pos.json', neg='neg.json', BestFeatureVec=num)\n",
    "    bernou.append(s.score(pos_test='pos.json', neg_test='neg.json'))\n",
    "\n",
    "plt.plot(range(10, 50, 10), multi, 'o-', color=\"y\",label=\"Multinomial\")\n",
    "plt.plot(range(10, 50, 10), bernou, 'o-', color=\"r\",label=\"Bernoulli\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"features vectors\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
